{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Initialize Project Structure and Dependencies",
        "description": "Set up the Python project with proper directory structure, virtual environment, and install all required dependencies including pandas, numpy, plotly, anthropic, pydantic, python-dotenv, and pytest.",
        "details": "Create the following directory structure:\n```\nproject-root/\n├── src/\n│   ├── __init__.py\n│   ├── data_processing/\n│   │   ├── __init__.py\n│   │   ├── ingestion.py\n│   │   └── validation.py\n│   ├── metrics/\n│   │   ├── __init__.py\n│   │   ├── basic_metrics.py\n│   │   ├── time_metrics.py\n│   │   └── anomaly_metrics.py\n│   ├── analysis/\n│   │   └── __init__.py\n│   ├── visualization/\n│   │   └── __init__.py\n│   └── reporting/\n│       └── __init__.py\n├── tests/\n│   ├── __init__.py\n│   ├── test_data_processing/\n│   ├── test_metrics/\n│   └── fixtures/\n├── requirements.txt\n├── .env.example\n├── .gitignore\n└── README.md\n```\n\nCreate requirements.txt:\n```\npandas==2.2.0\nnumpy==1.26.3\nplotly==5.18.0\nanthropicpython-dotenv==1.0.0\npydantic==2.5.3\npytest==7.4.4\npytest-cov==4.1.0\n```\n\nSet up virtual environment:\n```bash\npython3.13 -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install -r requirements.txt\n```",
        "testStrategy": "Verify project structure by running: `python -c \"import pandas, numpy, plotly, anthropic, pydantic, dotenv, pytest\"` to ensure all dependencies are installed. Create a simple test file that imports from each module directory to verify proper Python package structure.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Project Directory Structure",
            "description": "Create the complete directory structure with all required folders and subfolders as specified in the project layout.",
            "dependencies": [],
            "details": "Create the main project directory structure including src/ with subdirectories (data_processing, metrics, analysis, visualization, reporting), tests/ with test subdirectories, and ensure all directories are properly organized according to the specification.",
            "status": "done",
            "testStrategy": "Verify directory structure exists by checking all folders are created correctly and accessible"
          },
          {
            "id": 2,
            "title": "Initialize Python Module Structure",
            "description": "Create all required __init__.py files and empty Python module files to establish proper Python package structure.",
            "dependencies": [
              1
            ],
            "details": "Create __init__.py files in all directories to make them Python packages. Create empty Python files like ingestion.py, validation.py, basic_metrics.py, time_metrics.py, anomaly_metrics.py in their respective directories.",
            "status": "done",
            "testStrategy": "Test imports from each module directory to verify proper Python package structure"
          },
          {
            "id": 3,
            "title": "Create Requirements File with Dependencies",
            "description": "Create requirements.txt file with exact version specifications for all required Python packages.",
            "dependencies": [],
            "details": "Create requirements.txt with specified versions: pandas==2.2.0, numpy==1.26.3, plotly==5.18.0, anthropic, python-dotenv==1.0.0, pydantic==2.5.3, pytest==7.4.4, pytest-cov==4.1.0",
            "status": "done",
            "testStrategy": "Validate requirements.txt syntax and package availability"
          },
          {
            "id": 4,
            "title": "Set Up Virtual Environment",
            "description": "Create and configure Python virtual environment for the project using Python 3.13 and install all dependencies.",
            "dependencies": [
              3
            ],
            "details": "Create virtual environment using python3.13 -m venv venv, activate it, and install all packages from requirements.txt using pip install -r requirements.txt",
            "status": "done",
            "testStrategy": "Verify all dependencies are installed by running import test for each package: pandas, numpy, plotly, anthropic, pydantic, dotenv, pytest"
          },
          {
            "id": 5,
            "title": "Create Configuration Files",
            "description": "Create essential project configuration files including .env.example, .gitignore, and README.md.",
            "dependencies": [
              2
            ],
            "details": "Create .env.example with placeholder environment variables, .gitignore with Python-specific ignore patterns (venv/, __pycache__/, .env, *.pyc, etc.), and basic README.md with project description and setup instructions.",
            "status": "done",
            "testStrategy": "Verify all configuration files are created with appropriate content and proper formatting"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down project initialization into: 1) Create directory structure with all required modules, 2) Set up virtual environment configuration, 3) Create requirements.txt with exact version specifications, 4) Initialize empty Python modules with proper __init__.py files, 5) Create basic configuration files (.env.example, .gitignore, README.md)",
        "updatedAt": "2025-11-08T11:48:08.225Z"
      },
      {
        "id": 2,
        "title": "Implement Data Ingestion and Validation Module",
        "description": "Create the data processing module with CSV ingestion capabilities using pandas and data validation using pydantic models. Handle required columns, data types, and implement robust error handling.",
        "details": "In `src/data_processing/validation.py`:\n```python\nfrom pydantic import BaseModel, validator\nfrom typing import Optional\nimport pandas as pd\n\nclass MinerDataSchema(BaseModel):\n    class Config:\n        arbitrary_types_allowed = True\n    \n    seconds: pd.Series\n    mode_power: pd.Series\n    summary_wattage: pd.Series\n    temp_hash_board_max: pd.Series\n    psu_temp_max: pd.Series\n    outage: pd.Series\n    \n    @validator('seconds')\n    def validate_seconds(cls, v):\n        if v.dtype not in ['float64', 'int64']:\n            raise ValueError('seconds must be numeric')\n        return v\n```\n\nIn `src/data_processing/ingestion.py`:\n```python\nimport pandas as pd\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nlogger = logging.getLogger(__name__)\n\nclass DataIngestion:\n    REQUIRED_COLUMNS = [\n        'miner.seconds',\n        'miner.mode.power',\n        'miner.summary.wattage',\n        'miner.temp.hash_board_max',\n        'miner.psu.temp_max',\n        'miner.outage'\n    ]\n    \n    def load_csv(self, filepath: Path) -> pd.DataFrame:\n        \"\"\"Load and validate CSV file.\"\"\"\n        try:\n            df = pd.read_csv(filepath)\n            self._validate_columns(df)\n            df = self._standardize_column_names(df)\n            df = self._convert_types(df)\n            return df\n        except Exception as e:\n            logger.error(f\"Failed to load CSV: {e}\")\n            raise\n    \n    def _validate_columns(self, df: pd.DataFrame) -> None:\n        missing = set(self.REQUIRED_COLUMNS) - set(df.columns)\n        if missing:\n            raise ValueError(f\"Missing required columns: {missing}\")\n    \n    def _standardize_column_names(self, df: pd.DataFrame) -> pd.DataFrame:\n        rename_map = {\n            'miner.seconds': 'seconds',\n            'miner.mode.power': 'mode_power',\n            'miner.summary.wattage': 'summary_wattage',\n            'miner.temp.hash_board_max': 'temp_hash_board_max',\n            'miner.psu.temp_max': 'psu_temp_max',\n            'miner.outage': 'outage'\n        }\n        return df.rename(columns=rename_map)\n    \n    def _convert_types(self, df: pd.DataFrame) -> pd.DataFrame:\n        df['seconds'] = pd.to_numeric(df['seconds'], errors='coerce')\n        df['mode_power'] = pd.to_numeric(df['mode_power'], errors='coerce')\n        df['summary_wattage'] = pd.to_numeric(df['summary_wattage'], errors='coerce')\n        df['outage'] = df['outage'].astype(bool)\n        return df\n```",
        "testStrategy": "Create unit tests in `tests/test_data_processing/test_ingestion.py` that verify:\n1. Successful loading of valid CSV files\n2. Proper error handling for missing columns\n3. Correct type conversions\n4. Handling of malformed data\n5. Create fixture CSV files with various edge cases (missing columns, wrong types, empty files)",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "pending",
        "subtasks": [],
        "complexity": 5,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down into: 1) Implement CSV loading with pandas and column validation, 2) Create Pydantic data validation models with proper type checking, 3) Build robust error handling for malformed data and missing columns, 4) Implement data type conversion and standardization with comprehensive logging"
      },
      {
        "id": 3,
        "title": "Implement Data Preprocessing Pipeline",
        "description": "Create preprocessing functions to sort data chronologically, identify action time (t=0), handle NaN values, and prepare data for metric calculations.",
        "details": "In `src/data_processing/preprocessing.py`:\n```python\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, Tuple, Any\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass DataPreprocessor:\n    def __init__(self, df: pd.DataFrame):\n        self.df = df.copy()\n        self.action_index = None\n        self.metadata = {}\n    \n    def preprocess(self) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n        \"\"\"Main preprocessing pipeline.\"\"\"\n        self._sort_by_time()\n        self._identify_action_point()\n        self._check_data_quality()\n        self._handle_nan_values()\n        return self.df, self.metadata\n    \n    def _sort_by_time(self) -> None:\n        \"\"\"Sort data chronologically by seconds column.\"\"\"\n        self.df = self.df.sort_values('seconds').reset_index(drop=True)\n        logger.info(f\"Sorted {len(self.df)} rows by time\")\n    \n    def _identify_action_point(self) -> None:\n        \"\"\"Find the index where seconds crosses from negative to non-negative.\"\"\"\n        # Find transition point\n        negative_mask = self.df['seconds'] < 0\n        if not negative_mask.any():\n            logger.warning(\"No pre-action data found (all times >= 0)\")\n            self.action_index = 0\n        elif negative_mask.all():\n            logger.warning(\"No post-action data found (all times < 0)\")\n            self.action_index = len(self.df) - 1\n        else:\n            # Find first non-negative index\n            self.action_index = (~negative_mask).idxmax()\n        \n        self.metadata['action_index'] = self.action_index\n        self.metadata['action_time'] = self.df.loc[self.action_index, 'seconds']\n    \n    def _check_data_quality(self) -> None:\n        \"\"\"Assess data quality and log warnings.\"\"\"\n        # Check for NaN values\n        nan_counts = self.df.isnull().sum()\n        for col, count in nan_counts.items():\n            if count > 0:\n                pct = (count / len(self.df)) * 100\n                logger.warning(f\"Column '{col}' has {count} NaN values ({pct:.1f}%)\")\n                self.metadata[f'{col}_nan_count'] = count\n        \n        # Check for data gaps\n        time_diffs = self.df['seconds'].diff()\n        max_gap = time_diffs.max()\n        if max_gap > 10:  # More than 10 second gap\n            logger.warning(f\"Large time gap detected: {max_gap:.1f} seconds\")\n            self.metadata['max_time_gap'] = max_gap\n    \n    def _handle_nan_values(self) -> None:\n        \"\"\"Strategy for handling NaN values in wattage.\"\"\"\n        # For wattage, we'll keep NaN as-is but flag segments\n        wattage_nan_mask = self.df['summary_wattage'].isna()\n        if wattage_nan_mask.any():\n            # Find continuous NaN segments\n            nan_segments = []\n            in_segment = False\n            start_idx = None\n            \n            for idx, is_nan in enumerate(wattage_nan_mask):\n                if is_nan and not in_segment:\n                    start_idx = idx\n                    in_segment = True\n                elif not is_nan and in_segment:\n                    nan_segments.append((start_idx, idx-1))\n                    in_segment = False\n            \n            if in_segment:\n                nan_segments.append((start_idx, len(self.df)-1))\n            \n            self.metadata['wattage_nan_segments'] = nan_segments\n    \n    def get_pre_action_data(self) -> pd.DataFrame:\n        \"\"\"Return data before action point.\"\"\"\n        return self.df[self.df['seconds'] < 0].copy()\n    \n    def get_post_action_data(self) -> pd.DataFrame:\n        \"\"\"Return data at and after action point.\"\"\"\n        return self.df[self.df['seconds'] >= 0].copy()\n```",
        "testStrategy": "Create comprehensive tests in `tests/test_data_processing/test_preprocessing.py`:\n1. Test sorting with unsorted data\n2. Test action point identification with various scenarios (no pre-data, no post-data, normal)\n3. Test NaN handling and segment identification\n4. Test data quality checks and metadata generation\n5. Create fixtures with edge cases like all NaN values, single row data, large time gaps",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "pending",
        "subtasks": [],
        "complexity": 6,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down into: 1) Implement chronological sorting and time validation, 2) Build action point identification (t=0) logic with edge case handling, 3) Create data quality assessment and NaN analysis, 4) Implement metadata generation and logging system, 5) Build data segmentation methods (pre/post action data extraction)"
      },
      {
        "id": 4,
        "title": "Implement Basic Metrics: Start Power and Target Power",
        "description": "Create the first two basic metrics that calculate baseline power consumption and extract target power settings for transition analysis.",
        "details": "In `src/metrics/basic_metrics.py`:\n```python\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, Any, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass BasicMetrics:\n    def __init__(self, df: pd.DataFrame, metadata: Dict[str, Any]):\n        self.df = df\n        self.metadata = metadata\n        self.action_index = metadata.get('action_index', 0)\n    \n    def calculate_start_power(self) -> Dict[str, Any]:\n        \"\"\"Metric 1: Calculate baseline power consumption before action.\"\"\"\n        pre_action_data = self.df[self.df['seconds'] < 0]\n        \n        if pre_action_data.empty:\n            logger.warning(\"No pre-action data available for start power calculation\")\n            return {\n                'median': None,\n                'last_value': None,\n                'difference': None,\n                'notes': 'No pre-action data available'\n            }\n        \n        # Filter valid (non-NaN) wattage values\n        valid_wattage = pre_action_data['summary_wattage'].dropna()\n        \n        if valid_wattage.empty:\n            return {\n                'median': None,\n                'last_value': None,\n                'difference': None,\n                'notes': 'All pre-action wattage values are NaN'\n            }\n        \n        # Calculate median\n        median_power = valid_wattage.median()\n        \n        # Get last value before action\n        last_idx = pre_action_data.index[-1]\n        last_value = pre_action_data.loc[last_idx, 'summary_wattage']\n        \n        # Calculate difference if last value is not NaN\n        if pd.notna(last_value):\n            difference = abs(last_value - median_power)\n            notes = 'Normal' if difference <= 50 else f'Significant difference: {difference:.1f}W'\n        else:\n            difference = None\n            notes = 'Last value before action is NaN'\n        \n        return {\n            'median': round(median_power, 2),\n            'last_value': round(last_value, 2) if pd.notna(last_value) else None,\n            'difference': round(difference, 2) if difference is not None else None,\n            'notes': notes,\n            'valid_samples': len(valid_wattage),\n            'total_samples': len(pre_action_data)\n        }\n    \n    def calculate_target_power(self) -> Dict[str, Any]:\n        \"\"\"Metric 2: Extract target power settings for transition analysis.\"\"\"\n        # Get mode power immediately before action\n        pre_action_data = self.df[self.df['seconds'] < 0]\n        post_action_data = self.df[self.df['seconds'] >= 0]\n        \n        if pre_action_data.empty:\n            before_power = None\n            logger.warning(\"No pre-action data for target power\")\n        else:\n            before_power = pre_action_data.iloc[-1]['mode_power']\n        \n        if post_action_data.empty:\n            after_power = None\n            logger.warning(\"No post-action data for target power\")\n        else:\n            after_power = post_action_data.iloc[0]['mode_power']\n        \n        # Calculate change\n        if before_power is not None and after_power is not None:\n            change = after_power - before_power\n            \n            # Validate target remains constant post-action\n            post_power_values = post_action_data['mode_power'].dropna()\n            if not post_power_values.empty:\n                is_constant = post_power_values.nunique() == 1\n                if not is_constant:\n                    logger.warning(\"Target power changes during post-action period\")\n                    notes = 'Target power not constant after action'\n                elif abs(change) < 1:\n                    notes = 'Warning: No significant change detected'\n                else:\n                    notes = 'Normal transition'\n            else:\n                notes = 'Cannot validate post-action consistency'\n        else:\n            change = None\n            notes = 'Missing data for calculation'\n        \n        return {\n            'before': round(before_power, 2) if before_power is not None else None,\n            'after': round(after_power, 2) if after_power is not None else None,\n            'change': round(change, 2) if change is not None else None,\n            'notes': notes\n        }\n```",
        "testStrategy": "Create unit tests in `tests/test_metrics/test_basic_metrics.py`:\n1. Test start power with normal data (consistent pre-action values)\n2. Test start power with high variance data\n3. Test start power with all NaN values\n4. Test target power with normal step change\n5. Test target power with no change (minimal step)\n6. Test target power with inconsistent post-action values\n7. Test edge cases: empty dataframes, single row data",
        "priority": "high",
        "dependencies": [
          "3"
        ],
        "status": "pending",
        "subtasks": [],
        "complexity": 4,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down into: 1) Implement start power calculation with median computation and pre-action data analysis, 2) Implement target power extraction with before/after comparison and validation, 3) Add comprehensive error handling, edge cases, and validation logic for both metrics"
      },
      {
        "id": 5,
        "title": "Implement Step Direction and Temperature Metrics",
        "description": "Create step direction classification metric and temperature range analysis for thermal behavior during tests.",
        "details": "Continue in `src/metrics/basic_metrics.py`:\n```python\n    def calculate_step_direction(self, target_power_result: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Metric 3: Classify test type based on power transition.\"\"\"\n        # Requires target power metric results\n        if target_power_result['change'] is None:\n            return {\n                'direction': 'UNKNOWN',\n                'delta_watts': None,\n                'delta_percentage': None,\n                'notes': 'Cannot determine - missing target power data'\n            }\n        \n        change = target_power_result['change']\n        before = target_power_result['before']\n        \n        # Calculate percentage change\n        if before and before != 0:\n            pct_change = (change / before) * 100\n        else:\n            pct_change = 0\n        \n        # Apply classification thresholds\n        abs_change = abs(change)\n        abs_pct = abs(pct_change)\n        \n        if abs_change < 50 and abs_pct < 2:\n            direction = 'MINIMAL-STEP'\n        elif change > 0:\n            direction = 'UP-STEP'\n        else:\n            direction = 'DOWN-STEP'\n        \n        return {\n            'direction': direction,\n            'delta_watts': round(change, 2),\n            'delta_percentage': round(pct_change, 2),\n            'notes': f'{direction} detected with {abs_change:.0f}W change ({abs_pct:.1f}%)'\n        }\n    \n    def calculate_temperature_ranges(self) -> Dict[str, Any]:\n        \"\"\"Metric 4: Analyze thermal behavior during test.\"\"\"\n        pre_action = self.df[self.df['seconds'] < 0]\n        post_action = self.df[self.df['seconds'] >= 0]\n        \n        result = {\n            'pre_action': {},\n            'post_action': {},\n            'max_temperatures': {}\n        }\n        \n        # Analyze pre-action temperatures\n        if not pre_action.empty:\n            hash_temps_pre = pre_action['temp_hash_board_max'].dropna()\n            psu_temps_pre = pre_action['psu_temp_max'].dropna()\n            \n            if not hash_temps_pre.empty:\n                result['pre_action']['hash_board'] = {\n                    'min': round(hash_temps_pre.min(), 1),\n                    'max': round(hash_temps_pre.max(), 1),\n                    'mean': round(hash_temps_pre.mean(), 1),\n                    'std': round(hash_temps_pre.std(), 1)\n                }\n            \n            if not psu_temps_pre.empty:\n                result['pre_action']['psu'] = {\n                    'min': round(psu_temps_pre.min(), 1),\n                    'max': round(psu_temps_pre.max(), 1),\n                    'mean': round(psu_temps_pre.mean(), 1),\n                    'std': round(psu_temps_pre.std(), 1)\n                }\n        \n        # Analyze post-action temperatures\n        if not post_action.empty:\n            hash_temps_post = post_action['temp_hash_board_max'].dropna()\n            psu_temps_post = post_action['psu_temp_max'].dropna()\n            \n            if not hash_temps_post.empty:\n                result['post_action']['hash_board'] = {\n                    'min': round(hash_temps_post.min(), 1),\n                    'max': round(hash_temps_post.max(), 1),\n                    'mean': round(hash_temps_post.mean(), 1),\n                    'std': round(hash_temps_post.std(), 1)\n                }\n                \n                # Find max temperature timing\n                max_idx = post_action['temp_hash_board_max'].idxmax()\n                if pd.notna(max_idx):\n                    max_time = post_action.loc[max_idx, 'seconds']\n                    result['max_temperatures']['hash_board_time'] = round(max_time, 1)\n            \n            if not psu_temps_post.empty:\n                result['post_action']['psu'] = {\n                    'min': round(psu_temps_post.min(), 1),\n                    'max': round(psu_temps_post.max(), 1),\n                    'mean': round(psu_temps_post.mean(), 1),\n                    'std': round(psu_temps_post.std(), 1)\n                }\n                \n                # Find max PSU temperature timing\n                max_idx = post_action['psu_temp_max'].idxmax()\n                if pd.notna(max_idx):\n                    max_time = post_action.loc[max_idx, 'seconds']\n                    result['max_temperatures']['psu_time'] = round(max_time, 1)\n        \n        # Add temperature rise analysis\n        if result['pre_action'].get('hash_board') and result['post_action'].get('hash_board'):\n            temp_rise = result['post_action']['hash_board']['max'] - result['pre_action']['hash_board']['mean']\n            result['temperature_rise'] = round(temp_rise, 1)\n        \n        return result\n```",
        "testStrategy": "Extend tests in `tests/test_metrics/test_basic_metrics.py`:\n1. Test step direction for UP-STEP (>2% or >50W increase)\n2. Test step direction for DOWN-STEP (>2% or >50W decrease)\n3. Test step direction for MINIMAL-STEP (<2% and <50W)\n4. Test temperature ranges with complete data\n5. Test temperature ranges with missing temperature data\n6. Test temperature rise calculations\n7. Test max temperature timing identification",
        "priority": "high",
        "dependencies": [
          "4"
        ],
        "status": "pending",
        "subtasks": [],
        "complexity": 4,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down into: 1) Implement step direction classification logic using power deltas and thresholds, 2) Implement temperature range analysis with min/max calculations for both PSU and hash board sensors, 3) Add validation, edge case handling, and warning systems for temperature anomalies"
      },
      {
        "id": 6,
        "title": "Implement Time-Based Metrics: Band Entry and Setpoint Hit",
        "description": "Create metrics to measure time to reach target power band (±5%) and exact target power (±30W).",
        "details": "In `src/metrics/time_metrics.py`:\n```python\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, Any, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TimeMetrics:\n    def __init__(self, df: pd.DataFrame, metadata: Dict[str, Any]):\n        self.df = df\n        self.metadata = metadata\n    \n    def calculate_band_entry(self, target_power_result: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Metric 5: Measure time to reach target power band (±5%).\"\"\"\n        target = target_power_result.get('after')\n        \n        if target is None:\n            return {\n                'time_seconds': None,\n                'achieved': False,\n                'notes': 'No target power available'\n            }\n        \n        # Define band boundaries (±5%)\n        lower_bound = target * 0.95\n        upper_bound = target * 1.05\n        \n        # Get post-action data\n        post_action = self.df[self.df['seconds'] >= 0].copy()\n        \n        if post_action.empty:\n            return {\n                'time_seconds': None,\n                'achieved': False,\n                'notes': 'No post-action data'\n            }\n        \n        # Find first entry into band\n        valid_wattage = post_action.dropna(subset=['summary_wattage'])\n        in_band = (valid_wattage['summary_wattage'] >= lower_bound) & \\\n                  (valid_wattage['summary_wattage'] <= upper_bound)\n        \n        if not in_band.any():\n            return {\n                'time_seconds': None,\n                'achieved': False,\n                'notes': f'Never reached band [{lower_bound:.0f}, {upper_bound:.0f}]W',\n                'band_lower': round(lower_bound, 2),\n                'band_upper': round(upper_bound, 2)\n            }\n        \n        # Implement consecutive sample validation (at least 3 consecutive samples)\n        consecutive_count = 0\n        entry_time = None\n        \n        for idx in valid_wattage.index:\n            if in_band.loc[idx]:\n                consecutive_count += 1\n                if consecutive_count >= 3:\n                    # Go back to first of the 3 consecutive samples\n                    entry_idx = valid_wattage.index[valid_wattage.index.get_loc(idx) - 2]\n                    entry_time = valid_wattage.loc[entry_idx, 'seconds']\n                    break\n            else:\n                consecutive_count = 0\n        \n        if entry_time is not None:\n            return {\n                'time_seconds': round(entry_time, 2),\n                'achieved': True,\n                'notes': f'Reached band at {entry_time:.1f}s',\n                'band_lower': round(lower_bound, 2),\n                'band_upper': round(upper_bound, 2)\n            }\n        else:\n            return {\n                'time_seconds': None,\n                'achieved': False,\n                'notes': 'Did not maintain band for 3 consecutive samples',\n                'band_lower': round(lower_bound, 2),\n                'band_upper': round(upper_bound, 2)\n            }\n    \n    def calculate_setpoint_hit(self, target_power_result: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Metric 6: Measure time to reach exact target power (±30W).\"\"\"\n        target = target_power_result.get('after')\n        \n        if target is None:\n            return {\n                'time_seconds': None,\n                'achieved': False,\n                'notes': 'No target power available'\n            }\n        \n        # Define proximity threshold (±30W)\n        threshold = 30\n        lower_bound = target - threshold\n        upper_bound = target + threshold\n        \n        # Get post-action data\n        post_action = self.df[self.df['seconds'] >= 0].copy()\n        \n        if post_action.empty:\n            return {\n                'time_seconds': None,\n                'achieved': False,\n                'notes': 'No post-action data'\n            }\n        \n        # Find first occurrence within threshold\n        valid_wattage = post_action.dropna(subset=['summary_wattage'])\n        within_threshold = (valid_wattage['summary_wattage'] >= lower_bound) & \\\n                          (valid_wattage['summary_wattage'] <= upper_bound)\n        \n        if not within_threshold.any():\n            # Find closest approach\n            distances = abs(valid_wattage['summary_wattage'] - target)\n            if not distances.empty:\n                min_distance = distances.min()\n                closest_idx = distances.idxmin()\n                closest_time = valid_wattage.loc[closest_idx, 'seconds']\n                \n                return {\n                    'time_seconds': None,\n                    'achieved': False,\n                    'notes': f'Never reached within ±{threshold}W. Closest: {min_distance:.1f}W at {closest_time:.1f}s',\n                    'closest_distance': round(min_distance, 2),\n                    'closest_time': round(closest_time, 2)\n                }\n            else:\n                return {\n                    'time_seconds': None,\n                    'achieved': False,\n                    'notes': 'No valid wattage data'\n                }\n        \n        # Get first occurrence\n        first_hit_idx = within_threshold.idxmax()\n        hit_time = valid_wattage.loc[first_hit_idx, 'seconds']\n        actual_power = valid_wattage.loc[first_hit_idx, 'summary_wattage']\n        \n        return {\n            'time_seconds': round(hit_time, 2),\n            'achieved': True,\n            'notes': f'Reached {actual_power:.0f}W at {hit_time:.1f}s',\n            'actual_power': round(actual_power, 2),\n            'distance_from_target': round(abs(actual_power - target), 2)\n        }\n```",
        "testStrategy": "Create tests in `tests/test_metrics/test_time_metrics.py`:\n1. Test band entry with quick achievement\n2. Test band entry with delayed achievement\n3. Test band entry never achieved\n4. Test band entry with intermittent values (consecutive sample validation)\n5. Test setpoint hit with exact match\n6. Test setpoint hit within threshold\n7. Test setpoint hit never achieved with closest approach tracking",
        "priority": "medium",
        "dependencies": [
          "4"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement adaptive band tolerance calculation with multiple threshold modes",
            "description": "Create a flexible band tolerance system that can use percentage-based (±5%), absolute power-based (±30W), or hybrid thresholds for different scenarios and test types.",
            "dependencies": [],
            "details": "Extend the TimeMetrics class to support multiple band calculation modes: percentage-only, absolute-only, and adaptive hybrid that chooses the most restrictive. Add configuration options for different threshold values based on power levels and test scenarios. Include validation for threshold selection logic.",
            "status": "pending",
            "testStrategy": "Test with various power levels and threshold combinations to ensure proper band calculation in all modes",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Build continuous segment detection for band entry with 15-second dwell time validation",
            "description": "Implement sophisticated temporal analysis to detect when power enters and maintains the target band for a sustained 15-second period, replacing the simple 3-sample validation.",
            "dependencies": [
              1
            ],
            "details": "Replace the current consecutive sample logic with time-based validation that requires power to remain within band for at least 15 seconds continuously. Handle sampling rate variations and implement sliding window analysis to detect sustained band maintenance vs brief touches.",
            "status": "pending",
            "testStrategy": "Test with data containing brief band touches, sustained entries, and intermittent band maintenance patterns",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement comprehensive setpoint hit tracking with 25-second sustain logic",
            "description": "Enhance setpoint hit detection to track not just first occurrence but sustained achievement within ±30W tolerance for at least 25 seconds.",
            "dependencies": [
              1
            ],
            "details": "Modify calculate_setpoint_hit to track both initial hit time and sustained achievement time. Implement duration analysis to classify hits as brief touches vs sustained achievements. Add tracking for maximum sustained duration and stability metrics during sustained periods.",
            "status": "pending",
            "testStrategy": "Test with various hit patterns including brief touches, oscillating around target, and stable sustained hits",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create event classification system (brief touches vs sustained hits)",
            "description": "Build a classification framework to categorize power events as brief touches, oscillating behavior, or sustained achievements for both band entry and setpoint hits.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement event classification logic that analyzes temporal patterns to distinguish between different types of target achievement. Create categories like 'brief_touch', 'oscillating', 'sustained_stable', and 'sustained_variable' with appropriate thresholds and pattern recognition algorithms.",
            "status": "pending",
            "testStrategy": "Test classification accuracy with known event patterns and edge cases involving mixed behavior types",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Build closest approach analysis for failure cases",
            "description": "Implement comprehensive analysis for cases where targets are never achieved, including closest approach timing, duration of closest approach, and trend analysis.",
            "dependencies": [
              4
            ],
            "details": "Enhance failure case reporting with detailed closest approach metrics including minimum distance from target, time of closest approach, duration spent near target, and trend analysis (approaching vs retreating). Add statistical analysis of near-miss patterns and oscillation detection around targets.",
            "status": "pending",
            "testStrategy": "Test with various failure scenarios including steady approach, oscillating near-miss, and trending away patterns",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Add comprehensive edge case handling and validation",
            "description": "Implement robust error handling and validation for all edge cases including missing data, invalid targets, extreme values, and inconsistent timestamps.",
            "dependencies": [
              5
            ],
            "details": "Add comprehensive input validation, handle edge cases like missing target data, invalid power readings, timestamp inconsistencies, and extreme outliers. Implement graceful degradation and detailed error reporting. Add data quality checks and warnings for suspicious patterns or data issues.",
            "status": "pending",
            "testStrategy": "Test with malformed data, missing values, extreme outliers, and various data quality issues to ensure robust handling",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down into: 1) Implement adaptive band tolerance calculation with multiple threshold modes, 2) Build continuous segment detection for band entry with 15-second dwell time validation, 3) Implement comprehensive setpoint hit tracking with 25-second sustain logic, 4) Create event classification system (brief touches vs sustained hits), 5) Build closest approach analysis for failure cases, 6) Add comprehensive edge case handling and validation"
      },
      {
        "id": 7,
        "title": "Implement Stable Plateau and Anomaly Detection Metrics",
        "description": "Create stable plateau duration metric and implement sharp drops detection for power outages and significant drops.",
        "details": "Continue in `src/metrics/time_metrics.py` and create `src/metrics/anomaly_metrics.py`:\n\nIn `time_metrics.py`:\n```python\n    def calculate_stable_plateau(self, target_power_result: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Metric 7: Measure duration at stable target power.\"\"\"\n        target = target_power_result.get('after')\n        \n        if target is None:\n            return {\n                'duration_seconds': 0,\n                'start_time': None,\n                'end_time': None,\n                'achieved': False,\n                'notes': 'No target power available'\n            }\n        \n        # Define stability criteria (±2% for 30+ seconds)\n        tolerance = 0.02\n        min_duration = 30\n        lower_bound = target * (1 - tolerance)\n        upper_bound = target * (1 + tolerance)\n        \n        post_action = self.df[self.df['seconds'] >= 0].copy()\n        valid_data = post_action.dropna(subset=['summary_wattage'])\n        \n        if valid_data.empty:\n            return {\n                'duration_seconds': 0,\n                'start_time': None,\n                'end_time': None,\n                'achieved': False,\n                'notes': 'No valid post-action data'\n            }\n        \n        # Find longest stable segment\n        in_range = (valid_data['summary_wattage'] >= lower_bound) & \\\n                   (valid_data['summary_wattage'] <= upper_bound)\n        \n        longest_duration = 0\n        best_start = None\n        best_end = None\n        \n        # Scan for continuous segments\n        segment_start = None\n        for i in range(len(valid_data)):\n            idx = valid_data.index[i]\n            \n            if in_range.loc[idx]:\n                if segment_start is None:\n                    segment_start = valid_data.loc[idx, 'seconds']\n            else:\n                if segment_start is not None:\n                    segment_end = valid_data.iloc[i-1]['seconds']\n                    duration = segment_end - segment_start\n                    \n                    if duration > longest_duration:\n                        longest_duration = duration\n                        best_start = segment_start\n                        best_end = segment_end\n                    \n                    segment_start = None\n        \n        # Check if last segment is ongoing\n        if segment_start is not None:\n            segment_end = valid_data.iloc[-1]['seconds']\n            duration = segment_end - segment_start\n            \n            if duration > longest_duration:\n                longest_duration = duration\n                best_start = segment_start\n                best_end = segment_end\n        \n        achieved = longest_duration >= min_duration\n        \n        return {\n            'duration_seconds': round(longest_duration, 2),\n            'start_time': round(best_start, 2) if best_start is not None else None,\n            'end_time': round(best_end, 2) if best_end is not None else None,\n            'achieved': achieved,\n            'notes': f'Stable for {longest_duration:.1f}s' if achieved else f'Max stability: {longest_duration:.1f}s (< {min_duration}s required)',\n            'stability_band': [round(lower_bound, 2), round(upper_bound, 2)]\n        }\n```\n\nIn `src/metrics/anomaly_metrics.py`:\n```python\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, Any, List, Tuple\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass AnomalyMetrics:\n    def __init__(self, df: pd.DataFrame, metadata: Dict[str, Any]):\n        self.df = df\n        self.metadata = metadata\n    \n    def calculate_sharp_drops(self) -> Dict[str, Any]:\n        \"\"\"Metric 8: Detect power outages and significant drops.\"\"\"\n        # Thresholds\n        drop_threshold = 500  # Watts\n        time_window = 3  # seconds\n        \n        # Get post-action data\n        post_action = self.df[self.df['seconds'] >= 0].copy()\n        \n        drops = []\n        outage_periods = []\n        \n        # Detect drops in power\n        for i in range(1, len(post_action)):\n            curr_idx = post_action.index[i]\n            prev_idx = post_action.index[i-1]\n            \n            curr_power = post_action.loc[curr_idx, 'summary_wattage']\n            prev_power = post_action.loc[prev_idx, 'summary_wattage']\n            curr_time = post_action.loc[curr_idx, 'seconds']\n            prev_time = post_action.loc[prev_idx, 'seconds']\n            \n            # Check for outage flag\n            if post_action.loc[curr_idx, 'outage']:\n                if not outage_periods or curr_time - outage_periods[-1][1] > 1:\n                    outage_periods.append([curr_time, curr_time])\n                else:\n                    outage_periods[-1][1] = curr_time\n            \n            # Check for power drop\n            if pd.notna(curr_power) and pd.notna(prev_power):\n                time_diff = curr_time - prev_time\n                if time_diff <= time_window:\n                    power_drop = prev_power - curr_power\n                    if power_drop >= drop_threshold:\n                        drops.append({\n                            'time': round(curr_time, 2),\n                            'magnitude': round(power_drop, 2),\n                            'from_power': round(prev_power, 2),\n                            'to_power': round(curr_power, 2),\n                            'duration': round(time_diff, 2)\n                        })\n        \n        # Calculate total outage time\n        total_outage_time = sum(end - start for start, end in outage_periods)\n        \n        return {\n            'drop_count': len(drops),\n            'drops': drops[:5],  # Return up to 5 most significant\n            'outage_count': len(outage_periods),\n            'total_outage_seconds': round(total_outage_time, 2),\n            'outage_periods': [(round(s, 2), round(e, 2)) for s, e in outage_periods[:5]],\n            'notes': f'{len(drops)} drops detected, {len(outage_periods)} outage periods'\n        }\n```",
        "testStrategy": "Create comprehensive tests:\n1. Test stable plateau with long stable period\n2. Test stable plateau with multiple short periods\n3. Test stable plateau never achieved\n4. Test sharp drops detection with single drop\n5. Test sharp drops with multiple drops\n6. Test outage period detection\n7. Test combined drops and outages",
        "priority": "medium",
        "dependencies": [
          "6"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement stable plateau detection with ±20W tolerance and 30-second minimum duration",
            "description": "Create the stable plateau detection algorithm that identifies continuous periods where power remains within ±20W of target for at least 30 seconds.",
            "dependencies": [],
            "details": "Modify the existing calculate_stable_plateau method in time_metrics.py to use ±20W absolute tolerance instead of percentage-based tolerance. Update the algorithm to properly handle continuous segment detection with the new tolerance criteria. Ensure the method returns duration, start/end times, achievement status, and stability band information.",
            "status": "pending",
            "testStrategy": "Test with stable periods meeting criteria, multiple short periods not meeting duration requirement, and edge cases with no stable periods",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Build continuous segment analysis with exit reason classification",
            "description": "Enhance segment analysis to classify why plateau periods end and provide detailed exit reason information.",
            "dependencies": [
              1
            ],
            "details": "Add logic to track and classify segment exit reasons (exceeded tolerance, data gap, test end). Implement segment merging for brief interruptions and provide detailed statistics about plateau quality including variance and deviation patterns during stable periods.",
            "status": "pending",
            "testStrategy": "Test exit reason classification for tolerance violations, data gaps, and natural test endings with various plateau patterns",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement sharp drop detection using rolling 5-second windows and 15% thresholds",
            "description": "Create sharp drop detection algorithm using rolling time windows and percentage-based thresholds for more accurate anomaly detection.",
            "dependencies": [],
            "details": "Replace the existing simple drop detection with a rolling window approach that calculates power changes over 5-second windows. Use 15% threshold for significant drops and implement proper time-based analysis to avoid false positives from normal fluctuations.",
            "status": "pending",
            "testStrategy": "Test with gradual declines, sudden drops, and noisy data to validate 15% threshold effectiveness over 5-second windows",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Build overlapping event prevention and time-based deduplication",
            "description": "Implement sophisticated event deduplication to prevent reporting multiple events for the same underlying power anomaly.",
            "dependencies": [
              3
            ],
            "details": "Create logic to merge overlapping or closely spaced events within configurable time windows. Implement event priority scoring to keep the most significant event when multiple events overlap. Add configurable suppression periods after major events to prevent duplicate reporting.",
            "status": "pending",
            "testStrategy": "Test deduplication with overlapping events, consecutive drops, and various time window configurations to ensure proper event consolidation",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create summary statistics and worst-case analysis",
            "description": "Build comprehensive statistical analysis of detected anomalies including worst-case scenarios and trend analysis.",
            "dependencies": [
              4
            ],
            "details": "Implement statistical aggregation of all detected events including maximum drop magnitude, total anomaly time, frequency analysis, and severity classification. Add worst-case analysis that identifies the most significant events and their impact on system stability.",
            "status": "pending",
            "testStrategy": "Test statistical calculations with various event patterns and validate worst-case identification logic with known severe anomaly scenarios",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Add comprehensive validation and edge case handling",
            "description": "Implement robust error handling, input validation, and edge case management for all anomaly detection algorithms.",
            "dependencies": [
              5
            ],
            "details": "Add comprehensive input validation for all detection methods, handle edge cases like empty datasets, all-NaN values, and insufficient data periods. Implement graceful degradation when detection algorithms cannot run and provide meaningful error messages and fallback behaviors.",
            "status": "pending",
            "testStrategy": "Test with malformed data, empty datasets, extreme values, and various edge cases to ensure robust operation and appropriate error handling",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down into: 1) Implement stable plateau detection with ±20W tolerance and 30-second minimum duration, 2) Build continuous segment analysis with exit reason classification, 3) Implement sharp drop detection using rolling 5-second windows and 15% thresholds, 4) Build overlapping event prevention and time-based deduplication, 5) Create summary statistics and worst-case analysis, 6) Add comprehensive validation and edge case handling"
      },
      {
        "id": 8,
        "title": "Implement Spike Detection and Overshoot/Undershoot Metrics",
        "description": "Complete the anomaly detection metrics with spike detection for temporary power excursions and overshoot/undershoot analysis for transient response.",
        "details": "Continue in `src/metrics/anomaly_metrics.py`:\n```python\n    def calculate_spikes(self, start_power_result: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Metric 9: Detect temporary power excursions.\"\"\"\n        baseline = start_power_result.get('median')\n        \n        if baseline is None:\n            return {\n                'spike_count': 0,\n                'spikes': [],\n                'notes': 'No baseline power available'\n            }\n        \n        # Define spike thresholds (10% above/below baseline)\n        upper_threshold = baseline * 1.10\n        lower_threshold = baseline * 0.90\n        \n        # Use sliding window (3 samples)\n        window_size = 3\n        post_action = self.df[self.df['seconds'] >= 0].copy()\n        valid_data = post_action.dropna(subset=['summary_wattage'])\n        \n        spikes = []\n        \n        if len(valid_data) < window_size:\n            return {\n                'spike_count': 0,\n                'spikes': [],\n                'notes': 'Insufficient data for spike detection'\n            }\n        \n        # Sliding window detection\n        for i in range(len(valid_data) - window_size + 1):\n            window = valid_data.iloc[i:i+window_size]\n            window_mean = window['summary_wattage'].mean()\n            \n            # Check if middle point is a spike\n            mid_idx = window.index[1]\n            mid_power = window.loc[mid_idx, 'summary_wattage']\n            mid_time = window.loc[mid_idx, 'seconds']\n            \n            # Spike detection logic\n            if mid_power > upper_threshold or mid_power < lower_threshold:\n                # Verify it's temporary (surrounding values are closer to baseline)\n                first_power = window.iloc[0]['summary_wattage']\n                last_power = window.iloc[2]['summary_wattage']\n                \n                first_closer = abs(first_power - baseline) < abs(mid_power - baseline)\n                last_closer = abs(last_power - baseline) < abs(mid_power - baseline)\n                \n                if first_closer and last_closer:\n                    spike_type = 'up' if mid_power > baseline else 'down'\n                    spikes.append({\n                        'time': round(mid_time, 2),\n                        'power': round(mid_power, 2),\n                        'deviation': round(mid_power - baseline, 2),\n                        'deviation_pct': round((mid_power - baseline) / baseline * 100, 2),\n                        'type': spike_type\n                    })\n        \n        # Remove duplicate spikes (within 2 seconds)\n        filtered_spikes = []\n        for spike in spikes:\n            if not filtered_spikes or spike['time'] - filtered_spikes[-1]['time'] > 2:\n                filtered_spikes.append(spike)\n        \n        return {\n            'spike_count': len(filtered_spikes),\n            'spikes': filtered_spikes[:10],  # Return up to 10 spikes\n            'baseline_power': round(baseline, 2),\n            'thresholds': {\n                'upper': round(upper_threshold, 2),\n                'lower': round(lower_threshold, 2)\n            },\n            'notes': f'{len(filtered_spikes)} spikes detected'\n        }\n    \n    def calculate_overshoot_undershoot(self, target_power_result: Dict[str, Any], \n                                       step_direction_result: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Metric 10: Analyze transient response characteristics.\"\"\"\n        target = target_power_result.get('after')\n        direction = step_direction_result.get('direction')\n        \n        if target is None or direction == 'UNKNOWN':\n            return {\n                'detected': False,\n                'type': None,\n                'magnitude': None,\n                'time': None,\n                'duration': None,\n                'notes': 'Missing required data'\n            }\n        \n        # Dynamic thresholds (4% or 200W, whichever is larger)\n        pct_threshold = target * 0.04\n        abs_threshold = 200\n        threshold = max(pct_threshold, abs_threshold)\n        \n        post_action = self.df[self.df['seconds'] >= 0].copy()\n        valid_data = post_action.dropna(subset=['summary_wattage'])\n        \n        if valid_data.empty:\n            return {\n                'detected': False,\n                'type': None,\n                'magnitude': None,\n                'time': None,\n                'duration': None,\n                'notes': 'No valid post-action data'\n            }\n        \n        # Detect based on step direction\n        if direction == 'UP-STEP':\n            # Look for overshoot (power > target + threshold)\n            overshoot_mask = valid_data['summary_wattage'] > (target + threshold)\n            if overshoot_mask.any():\n                # Find maximum overshoot\n                max_idx = valid_data.loc[overshoot_mask, 'summary_wattage'].idxmax()\n                max_power = valid_data.loc[max_idx, 'summary_wattage']\n                max_time = valid_data.loc[max_idx, 'seconds']\n                \n                # Calculate duration (how long above threshold)\n                duration = 0\n                for idx in valid_data.index:\n                    if valid_data.loc[idx, 'summary_wattage'] > (target + threshold):\n                        duration += 1\n                    elif idx > max_idx:\n                        break\n                \n                return {\n                    'detected': True,\n                    'type': 'overshoot',\n                    'magnitude': round(max_power - target, 2),\n                    'magnitude_pct': round((max_power - target) / target * 100, 2),\n                    'time': round(max_time, 2),\n                    'duration': duration,\n                    'peak_power': round(max_power, 2),\n                    'threshold': round(target + threshold, 2),\n                    'notes': f'Overshoot of {max_power - target:.0f}W at {max_time:.1f}s'\n                }\n        \n        elif direction == 'DOWN-STEP':\n            # Look for undershoot (power < target - threshold)\n            undershoot_mask = valid_data['summary_wattage'] < (target - threshold)\n            if undershoot_mask.any():\n                # Find minimum undershoot\n                min_idx = valid_data.loc[undershoot_mask, 'summary_wattage'].idxmin()\n                min_power = valid_data.loc[min_idx, 'summary_wattage']\n                min_time = valid_data.loc[min_idx, 'seconds']\n                \n                # Calculate duration\n                duration = 0\n                for idx in valid_data.index:\n                    if valid_data.loc[idx, 'summary_wattage'] < (target - threshold):\n                        duration += 1\n                    elif idx > min_idx:\n                        break\n                \n                return {\n                    'detected': True,\n                    'type': 'undershoot',\n                    'magnitude': round(target - min_power, 2),\n                    'magnitude_pct': round((target - min_power) / target * 100, 2),\n                    'time': round(min_time, 2),\n                    'duration': duration,\n                    'trough_power': round(min_power, 2),\n                    'threshold': round(target - threshold, 2),\n                    'notes': f'Undershoot of {target - min_power:.0f}W at {min_time:.1f}s'\n                }\n        \n        return {\n            'detected': False,\n            'type': None,\n            'magnitude': None,\n            'time': None,\n            'duration': None,\n            'notes': f'No transient detected for {direction}'\n        }\n```",
        "testStrategy": "Complete anomaly metric tests:\n1. Test spike detection with clear up/down spikes\n2. Test spike filtering (removing duplicates)\n3. Test spike detection with noisy data\n4. Test overshoot detection for UP-STEP\n5. Test undershoot detection for DOWN-STEP\n6. Test no transient detection for MINIMAL-STEP\n7. Test magnitude and duration calculations",
        "priority": "medium",
        "dependencies": [
          "7"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement rolling 5-second window spike detection with 15% threshold",
            "description": "Replace the existing 3-sample sliding window with a proper 5-second rolling window implementation using 15% deviation threshold for spike detection.",
            "dependencies": [],
            "details": "Modify the calculate_spikes method to use a 5-second time-based window instead of 3-sample window. Change threshold from 10% to 15% (upper_threshold = baseline * 1.15, lower_threshold = baseline * 0.85). Implement proper time-based windowing by filtering data within 5-second intervals and calculating rolling statistics.",
            "status": "pending",
            "testStrategy": "Test with synthetic data containing clear 5-second spikes at 15% and 20% deviations to verify detection accuracy and threshold boundaries.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Build overlapping spike prevention with processed time tracking",
            "description": "Implement sophisticated spike deduplication logic to prevent detection of overlapping spikes by tracking processed time intervals.",
            "dependencies": [
              1
            ],
            "details": "Create a processed_intervals list to track already-processed time ranges. Before adding a spike, check if its time window overlaps with any existing processed interval. Extend the current 2-second gap logic to use a configurable window (default 3 seconds) and maintain state of processed regions to avoid duplicate detections in overlapping windows.",
            "status": "pending",
            "testStrategy": "Test with data containing multiple closely-spaced spikes to ensure only unique, non-overlapping events are detected.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement overshoot detection for UP-STEP transitions with dynamic thresholds",
            "description": "Enhance the overshoot detection logic for UP-STEP transitions with improved dynamic threshold calculation and peak detection.",
            "dependencies": [],
            "details": "Refine the overshoot detection in calculate_overshoot_undershoot method. Implement adaptive thresholds based on target power magnitude (minimum 4% or 200W, whichever is larger). Add peak detection algorithm to find the maximum overshoot point and improve the duration calculation to measure time above threshold more accurately.",
            "status": "pending",
            "testStrategy": "Test with UP-STEP data containing clear overshoots of varying magnitudes (5%, 10%, 15%) to verify threshold adaptation and peak detection accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Build undershoot detection for DOWN-STEP transitions with magnitude analysis",
            "description": "Implement robust undershoot detection for DOWN-STEP transitions with precise magnitude calculation and trough identification.",
            "dependencies": [],
            "details": "Complete the undershoot detection logic in calculate_overshoot_undershoot method. Implement trough detection to find minimum power points during undershoot events. Add magnitude analysis with both absolute (watts) and relative (percentage) calculations. Improve duration measurement to track how long power remains below the threshold.",
            "status": "pending",
            "testStrategy": "Test with DOWN-STEP data containing undershoots of different magnitudes to verify trough detection and magnitude calculation accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create duration and magnitude calculations for transient events",
            "description": "Implement comprehensive duration tracking and magnitude analysis for both overshoot and undershoot transient events.",
            "dependencies": [
              3,
              4
            ],
            "details": "Create helper methods for calculating transient event durations by tracking consecutive samples above/below thresholds. Implement magnitude calculations in both absolute watts and percentage terms. Add time-to-peak and recovery time measurements. Include statistical measures like peak/trough power, average deviation during event, and settling time.",
            "status": "pending",
            "testStrategy": "Test duration calculations with events of known lengths (2s, 5s, 10s) and verify magnitude calculations against known overshoot/undershoot values.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement direction-specific logic based on step classification",
            "description": "Create robust direction-specific analysis logic that properly handles UP-STEP, DOWN-STEP, and MINIMAL-STEP classifications.",
            "dependencies": [
              3,
              4
            ],
            "details": "Enhance the direction-based logic to handle edge cases where step direction is ambiguous or MINIMAL-STEP. Add logic to skip transient analysis for MINIMAL-STEP cases. Implement direction validation to ensure overshoot detection only runs for UP-STEP and undershoot only for DOWN-STEP. Add cross-validation between step direction results and detected transients.",
            "status": "pending",
            "testStrategy": "Test with all three step direction types (UP-STEP, DOWN-STEP, MINIMAL-STEP) to verify appropriate transient analysis is applied or skipped.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Add comprehensive validation and edge case handling for all detection modes",
            "description": "Implement robust error handling, input validation, and edge case management for all spike and transient detection functions.",
            "dependencies": [
              1,
              2,
              5,
              6
            ],
            "details": "Add comprehensive input validation for all methods including null/empty data checks, invalid baseline values, and malformed target power results. Implement edge case handling for insufficient data, all-NaN segments, extreme outliers, and boundary conditions. Add detailed logging and error messages with specific guidance for each failure mode. Include data quality checks and warnings for suspicious patterns.",
            "status": "pending",
            "testStrategy": "Test with edge cases including empty datasets, all-NaN data, extreme outliers, missing baselines, and invalid step directions to ensure graceful handling.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Break down into: 1) Implement spike detection using rolling 5-second windows with 15% threshold logic, 2) Build overlapping spike prevention with processed time tracking, 3) Implement overshoot detection for UP-STEP transitions with dynamic thresholds, 4) Build undershoot detection for DOWN-STEP transitions with magnitude analysis, 5) Create duration and magnitude calculations for transient events, 6) Implement direction-specific logic based on step classification, 7) Add comprehensive validation and edge case handling for all detection modes"
      },
      {
        "id": 9,
        "title": "Create Metric Orchestrator and Dependency Management",
        "description": "Build a central orchestrator that manages metric calculation order based on dependencies and aggregates all results.",
        "details": "Create `src/metrics/orchestrator.py`:\n```python\nimport pandas as pd\nfrom typing import Dict, Any, List\nimport logging\nfrom datetime import datetime\n\nfrom src.data_processing.ingestion import DataIngestion\nfrom src.data_processing.preprocessing import DataPreprocessor\nfrom src.metrics.basic_metrics import BasicMetrics\nfrom src.metrics.time_metrics import TimeMetrics\nfrom src.metrics.anomaly_metrics import AnomalyMetrics\n\nlogger = logging.getLogger(__name__)\n\nclass MetricOrchestrator:\n    \"\"\"Orchestrates metric calculation with dependency management.\"\"\"\n    \n    def __init__(self):\n        self.results = {}\n        self.metadata = {}\n        self.execution_order = [\n            'preprocessing',\n            'start_power',\n            'target_power',\n            'step_direction',\n            'temperature_ranges',\n            'band_entry',\n            'setpoint_hit',\n            'stable_plateau',\n            'sharp_drops',\n            'spikes',\n            'overshoot_undershoot'\n        ]\n    \n    def process_file(self, filepath: str) -> Dict[str, Any]:\n        \"\"\"Process CSV file and calculate all metrics.\"\"\"\n        start_time = datetime.now()\n        \n        try:\n            # Data ingestion\n            logger.info(f\"Loading file: {filepath}\")\n            ingestion = DataIngestion()\n            df = ingestion.load_csv(filepath)\n            \n            # Preprocessing\n            logger.info(\"Preprocessing data\")\n            preprocessor = DataPreprocessor(df)\n            processed_df, preprocessing_metadata = preprocessor.preprocess()\n            \n            self.metadata.update(preprocessing_metadata)\n            self.metadata['filename'] = filepath\n            self.metadata['total_rows'] = len(processed_df)\n            \n            # Initialize metric calculators\n            basic_metrics = BasicMetrics(processed_df, self.metadata)\n            time_metrics = TimeMetrics(processed_df, self.metadata)\n            anomaly_metrics = AnomalyMetrics(processed_df, self.metadata)\n            \n            # Calculate metrics in dependency order\n            logger.info(\"Calculating metrics\")\n            \n            # Basic metrics\n            self.results['start_power'] = basic_metrics.calculate_start_power()\n            self.results['target_power'] = basic_metrics.calculate_target_power()\n            \n            # Step direction (depends on target_power)\n            self.results['step_direction'] = basic_metrics.calculate_step_direction(\n                self.results['target_power']\n            )\n            \n            # Temperature (independent)\n            self.results['temperature_ranges'] = basic_metrics.calculate_temperature_ranges()\n            \n            # Time-based metrics (depend on target_power)\n            self.results['band_entry'] = time_metrics.calculate_band_entry(\n                self.results['target_power']\n            )\n            self.results['setpoint_hit'] = time_metrics.calculate_setpoint_hit(\n                self.results['target_power']\n            )\n            self.results['stable_plateau'] = time_metrics.calculate_stable_plateau(\n                self.results['target_power']\n            )\n            \n            # Anomaly metrics\n            self.results['sharp_drops'] = anomaly_metrics.calculate_sharp_drops()\n            self.results['spikes'] = anomaly_metrics.calculate_spikes(\n                self.results['start_power']\n            )\n            self.results['overshoot_undershoot'] = anomaly_metrics.calculate_overshoot_undershoot(\n                self.results['target_power'],\n                self.results['step_direction']\n            )\n            \n            # Calculate processing time\n            end_time = datetime.now()\n            processing_time = (end_time - start_time).total_seconds()\n            \n            # Compile final results\n            return {\n                'success': True,\n                'metrics': self.results,\n                'metadata': {\n                    **self.metadata,\n                    'processing_time_seconds': round(processing_time, 3),\n                    'timestamp': datetime.now().isoformat()\n                },\n                'raw_data': processed_df.to_dict('records')  # For visualization\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error processing file: {e}\", exc_info=True)\n            return {\n                'success': False,\n                'error': str(e),\n                'error_type': type(e).__name__,\n                'metadata': self.metadata\n            }\n    \n    def validate_results(self) -> Dict[str, List[str]]:\n        \"\"\"Validate calculated metrics for consistency.\"\"\"\n        warnings = []\n        errors = []\n        \n        # Check if all expected metrics were calculated\n        expected_metrics = self.execution_order[1:]  # Exclude preprocessing\n        for metric in expected_metrics:\n            if metric not in self.results:\n                errors.append(f\"Missing metric: {metric}\")\n        \n        # Validate metric relationships\n        if 'start_power' in self.results and 'target_power' in self.results:\n            start = self.results['start_power'].get('median')\n            target_before = self.results['target_power'].get('before')\n            \n            if start and target_before and abs(start - target_before) > 100:\n                warnings.append(\n                    f\"Large discrepancy between start power ({start:.0f}W) \"\n                    f\"and target before ({target_before:.0f}W)\"\n                )\n        \n        # Check time metric consistency\n        if 'band_entry' in self.results and 'setpoint_hit' in self.results:\n            band_time = self.results['band_entry'].get('time_seconds')\n            setpoint_time = self.results['setpoint_hit'].get('time_seconds')\n            \n            if band_time and setpoint_time and setpoint_time < band_time:\n                warnings.append(\n                    f\"Setpoint hit ({setpoint_time}s) before band entry ({band_time}s)\"\n                )\n        \n        return {\n            'warnings': warnings,\n            'errors': errors,\n            'valid': len(errors) == 0\n        }\n```",
        "testStrategy": "Create integration tests in `tests/test_metrics/test_orchestrator.py`:\n1. Test full pipeline with clean data\n2. Test dependency resolution\n3. Test error handling for bad files\n4. Test result validation\n5. Test processing time measurement\n6. Create sample CSV files representing different test scenarios\n7. Verify all metrics are calculated in correct order",
        "priority": "high",
        "dependencies": [
          "8"
        ],
        "status": "pending",
        "subtasks": [],
        "complexity": 6,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down into: 1) Build dependency resolution system with proper execution ordering, 2) Implement error handling and rollback mechanisms for failed calculations, 3) Create result aggregation and validation system, 4) Build processing time tracking and metadata management, 5) Implement comprehensive validation with cross-metric consistency checks"
      },
      {
        "id": 10,
        "title": "Implement Comprehensive Testing Framework",
        "description": "Create a complete testing framework with unit tests for all metrics, integration tests for the orchestrator, and test fixtures with sample data.",
        "details": "Create test fixtures in `tests/fixtures/`:\n\n`tests/fixtures/sample_data.py`:\n```python\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\ndef create_upstep_test_data():\n    \"\"\"Create sample data for UP-STEP test.\"\"\"\n    # Pre-action: stable at 2000W\n    pre_times = np.arange(-300, 0, 1)\n    pre_power = np.random.normal(2000, 10, len(pre_times))\n    pre_mode = np.full(len(pre_times), 2000)\n    \n    # Post-action: step to 3000W with overshoot\n    post_times = np.arange(0, 600, 1)\n    post_power = np.zeros(len(post_times))\n    \n    # Transient response\n    for i, t in enumerate(post_times):\n        if t < 10:\n            post_power[i] = 2000 + (3000-2000) * (t/10)\n        elif t < 20:\n            post_power[i] = 3000 + 200 * np.exp(-(t-10)/5)  # Overshoot\n        else:\n            post_power[i] = np.random.normal(3000, 15, 1)[0]\n    \n    post_mode = np.full(len(post_times), 3000)\n    \n    # Combine data\n    df = pd.DataFrame({\n        'miner.seconds': np.concatenate([pre_times, post_times]),\n        'miner.mode.power': np.concatenate([pre_mode, post_mode]),\n        'miner.summary.wattage': np.concatenate([pre_power, post_power]),\n        'miner.temp.hash_board_max': np.random.normal(65, 2, len(pre_times) + len(post_times)),\n        'miner.psu.temp_max': np.random.normal(55, 2, len(pre_times) + len(post_times)),\n        'miner.outage': np.zeros(len(pre_times) + len(post_times), dtype=bool)\n    })\n    \n    return df\n\ndef create_downstep_with_drops_data():\n    \"\"\"Create sample data for DOWN-STEP with power drops.\"\"\"\n    # Similar structure but with drops\n    df = create_upstep_test_data()\n    df['miner.mode.power'] = df['miner.mode.power'].apply(lambda x: 3000 if x == 2000 else 2000)\n    df['miner.summary.wattage'] = 5000 - df['miner.summary.wattage']  # Invert\n    \n    # Add some drops\n    drop_indices = [350, 450, 550]\n    for idx in drop_indices:\n        if idx < len(df):\n            df.loc[idx:idx+5, 'miner.summary.wattage'] = 0\n            df.loc[idx:idx+5, 'miner.outage'] = True\n    \n    return df\n\ndef save_test_fixtures():\n    \"\"\"Save test CSV files.\"\"\"\n    fixtures_dir = Path(__file__).parent\n    \n    # Save different test scenarios\n    scenarios = {\n        'upstep_clean.csv': create_upstep_test_data(),\n        'downstep_with_drops.csv': create_downstep_with_drops_data(),\n    }\n    \n    for filename, df in scenarios.items():\n        df.to_csv(fixtures_dir / filename, index=False)\n```\n\nCreate `tests/conftest.py` for pytest fixtures:\n```python\nimport pytest\nimport pandas as pd\nfrom pathlib import Path\n\n@pytest.fixture\ndef sample_upstep_df():\n    \"\"\"Provide sample UP-STEP dataframe.\"\"\"\n    from tests.fixtures.sample_data import create_upstep_test_data\n    return create_upstep_test_data()\n\n@pytest.fixture\ndef sample_downstep_df():\n    \"\"\"Provide sample DOWN-STEP dataframe.\"\"\"\n    from tests.fixtures.sample_data import create_downstep_with_drops_data\n    return create_downstep_with_drops_data()\n\n@pytest.fixture\ndef temp_csv_file(tmp_path, sample_upstep_df):\n    \"\"\"Create temporary CSV file for testing.\"\"\"\n    csv_path = tmp_path / \"test_data.csv\"\n    sample_upstep_df.to_csv(csv_path, index=False)\n    return str(csv_path)\n```\n\nCreate comprehensive test example `tests/test_metrics/test_basic_metrics.py`:\n```python\nimport pytest\nimport pandas as pd\nimport numpy as np\nfrom src.metrics.basic_metrics import BasicMetrics\n\nclass TestStartPower:\n    def test_normal_calculation(self, sample_upstep_df):\n        \"\"\"Test start power with normal data.\"\"\"\n        metrics = BasicMetrics(sample_upstep_df, {'action_index': 300})\n        result = metrics.calculate_start_power()\n        \n        assert result['median'] is not None\n        assert 1990 < result['median'] < 2010  # Should be close to 2000W\n        assert result['achieved'] is True\n        assert 'valid_samples' in result\n    \n    def test_no_pre_action_data(self):\n        \"\"\"Test when no pre-action data exists.\"\"\"\n        df = pd.DataFrame({\n            'seconds': [0, 1, 2],\n            'summary_wattage': [1000, 1000, 1000],\n            'mode_power': [1000, 1000, 1000]\n        })\n        metrics = BasicMetrics(df, {'action_index': 0})\n        result = metrics.calculate_start_power()\n        \n        assert result['median'] is None\n        assert result['notes'] == 'No pre-action data available'\n    \n    def test_all_nan_values(self):\n        \"\"\"Test when all pre-action values are NaN.\"\"\"\n        df = pd.DataFrame({\n            'seconds': [-3, -2, -1, 0, 1],\n            'summary_wattage': [np.nan, np.nan, np.nan, 1000, 1000],\n            'mode_power': [1000, 1000, 1000, 1000, 1000]\n        })\n        metrics = BasicMetrics(df, {'action_index': 3})\n        result = metrics.calculate_start_power()\n        \n        assert result['median'] is None\n        assert 'All pre-action wattage values are NaN' in result['notes']\n```\n\nCreate `tests/test_integration.py`:\n```python\nimport pytest\nfrom src.metrics.orchestrator import MetricOrchestrator\n\ndef test_full_pipeline(temp_csv_file):\n    \"\"\"Test complete metric calculation pipeline.\"\"\"\n    orchestrator = MetricOrchestrator()\n    result = orchestrator.process_file(temp_csv_file)\n    \n    assert result['success'] is True\n    assert 'metrics' in result\n    assert 'metadata' in result\n    \n    # Verify all metrics calculated\n    expected_metrics = [\n        'start_power', 'target_power', 'step_direction',\n        'temperature_ranges', 'band_entry', 'setpoint_hit',\n        'stable_plateau', 'sharp_drops', 'spikes',\n        'overshoot_undershoot'\n    ]\n    \n    for metric in expected_metrics:\n        assert metric in result['metrics']\n    \n    # Validate processing time\n    assert result['metadata']['processing_time_seconds'] < 1.0\n```",
        "testStrategy": "Run comprehensive test suite:\n1. Execute `pytest -v` to run all tests\n2. Use `pytest --cov=src` for coverage report\n3. Target >90% code coverage\n4. Run `pytest -k test_metric_name` for specific metric tests\n5. Use `pytest --lf` to rerun failed tests\n6. Create GitHub Actions workflow for CI/CD\n7. Add performance benchmarks with pytest-benchmark",
        "priority": "high",
        "dependencies": [
          "9"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create synthetic test data generators with comprehensive scenarios",
            "description": "Build robust test data generators in tests/fixtures/sample_data.py that create realistic time-series data for UP-STEP, DOWN-STEP, and edge case scenarios including power drops, temperature variations, and outages.",
            "dependencies": [],
            "details": "Implement create_upstep_test_data(), create_downstep_with_drops_data(), and additional generators for edge cases like minimal steps, high noise, and missing data segments. Include realistic thermal profiles and PSU temperature data. Add save_test_fixtures() function to persist CSV files for testing.",
            "status": "pending",
            "testStrategy": "Validate generated data meets expected statistical properties and contains proper pre/post action segments with realistic transient responses",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Build comprehensive unit test suites for basic metrics",
            "description": "Create exhaustive unit tests for start_power and target_power metrics covering normal operation, edge cases, and error conditions with multiple test scenarios per metric.",
            "dependencies": [
              1
            ],
            "details": "Implement test_basic_metrics.py with TestStartPower and TestTargetPower classes. Cover normal calculations, no pre-action data, all NaN values, high variance data, and boundary conditions. Test both successful calculations and error handling with proper assertions for median values, achieved status, and metadata.",
            "status": "pending",
            "testStrategy": "Run pytest -v tests/test_metrics/test_basic_metrics.py and achieve >95% coverage for basic_metrics.py module",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create unit tests for step direction and temperature metrics",
            "description": "Develop comprehensive test coverage for step direction classification and temperature range analysis with various power transition scenarios and thermal data patterns.",
            "dependencies": [
              1
            ],
            "details": "Extend test_basic_metrics.py with TestStepDirection and TestTemperatureRanges classes. Test UP-STEP (>2% or >50W increase), DOWN-STEP (>2% or >50W decrease), MINIMAL-STEP cases, and temperature analysis with complete/incomplete thermal data. Include boundary testing for percentage and absolute thresholds.",
            "status": "pending",
            "testStrategy": "Verify correct classification of all step types and accurate temperature statistics with edge case handling",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement unit tests for anomaly detection metrics",
            "description": "Build complete test suites for spike detection, sharp drops, and overshoot/undershoot metrics with various anomaly patterns and noise levels in the data.",
            "dependencies": [
              1
            ],
            "details": "Create test_anomaly_metrics.py with comprehensive coverage of spike detection algorithms, drop identification, and transient response analysis. Test spike filtering, baseline calculations, overshoot detection for UP-STEP, undershoot for DOWN-STEP, and no transient cases for MINIMAL-STEP scenarios.",
            "status": "pending",
            "testStrategy": "Validate anomaly detection accuracy with controlled synthetic anomalies and verify proper handling of edge cases",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create integration tests for full orchestrator pipeline",
            "description": "Develop end-to-end integration tests that verify the complete metric calculation pipeline from CSV input to final results output with proper error handling and performance validation.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Implement test_integration.py with full pipeline testing using MetricOrchestrator. Test complete workflow from file loading through all metric calculations, validate result structure contains all expected metrics, verify processing time requirements, and test error propagation and recovery.",
            "status": "pending",
            "testStrategy": "Run full pipeline tests with pytest and validate processing time <1.0 seconds and success rate >99% across test scenarios",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Set up pytest configuration and performance testing framework",
            "description": "Configure pytest environment with fixtures, create conftest.py for shared test utilities, and implement performance regression tests to ensure metric calculations remain efficient.",
            "dependencies": [
              1
            ],
            "details": "Create conftest.py with sample_upstep_df, sample_downstep_df, and temp_csv_file fixtures. Set up pytest.ini configuration, implement performance benchmarking tests, create GitHub Actions workflow for CI/CD, and establish coverage reporting with target >90% code coverage across all modules.",
            "status": "pending",
            "testStrategy": "Execute pytest --cov=src to validate coverage targets and run performance benchmarks to establish baseline metrics",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down into: 1) Create synthetic test data generators for different scenarios (UP-STEP, DOWN-STEP, edge cases), 2) Build unit test suites for each individual metric with comprehensive edge case coverage, 3) Create integration tests for the full orchestrator pipeline, 4) Implement test fixtures with pytest configuration and data management, 5) Build performance and regression test suites, 6) Create visual validation tools and reporting for metric verification"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-11-08T11:48:08.226Z",
      "taskCount": 10,
      "completedCount": 0,
      "tags": [
        "master"
      ],
      "created": "2025-11-08T11:52:16.561Z",
      "description": "Tasks for master context",
      "updated": "2025-11-08T11:56:37.073Z"
    }
  }
}